{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f81c4319510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "\n",
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data loader\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(28), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=4096,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        channels, img_size = 1, 28\n",
    "        latent_dim = 100\n",
    "        self.image_shape = (channels, img_size, img_size)\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),   # * means unpacking (list, tuple)\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, np.prod(self.image_shape)),   # 784\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        img = self.model(x)\n",
    "        img = img.view(img.size(0), *self.image_shape)\n",
    "        return img\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "n_epochs = 200\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator()\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "\n",
    "if cuda:\n",
    "    G.cuda()\n",
    "    D.cuda()\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(b1, b2))\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(b1, b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [D loss: 0.423126] [G loss: 0.577006]\n",
      "[Epoch 1/200] [D loss: 0.503467] [G loss: 0.608587]\n",
      "[Epoch 2/200] [D loss: 0.460986] [G loss: 0.753251]\n",
      "[Epoch 3/200] [D loss: 0.478365] [G loss: 0.756156]\n",
      "[Epoch 4/200] [D loss: 0.553855] [G loss: 0.609741]\n",
      "[Epoch 5/200] [D loss: 0.361735] [G loss: 0.949578]\n",
      "[Epoch 6/200] [D loss: 0.491487] [G loss: 0.758932]\n",
      "[Epoch 7/200] [D loss: 0.561717] [G loss: 0.657450]\n",
      "[Epoch 8/200] [D loss: 0.691802] [G loss: 0.628700]\n",
      "[Epoch 9/200] [D loss: 0.613466] [G loss: 0.643305]\n",
      "[Epoch 10/200] [D loss: 0.656488] [G loss: 0.556321]\n",
      "[Epoch 11/200] [D loss: 0.712181] [G loss: 0.696349]\n",
      "[Epoch 12/200] [D loss: 0.626507] [G loss: 0.641300]\n",
      "[Epoch 13/200] [D loss: 0.619393] [G loss: 0.657371]\n",
      "[Epoch 14/200] [D loss: 0.566474] [G loss: 1.274426]\n",
      "[Epoch 15/200] [D loss: 0.443301] [G loss: 0.940071]\n",
      "[Epoch 16/200] [D loss: 0.457194] [G loss: 0.918292]\n",
      "[Epoch 17/200] [D loss: 0.467228] [G loss: 1.117216]\n",
      "[Epoch 18/200] [D loss: 0.506735] [G loss: 0.880343]\n",
      "[Epoch 19/200] [D loss: 0.556055] [G loss: 0.591902]\n",
      "[Epoch 20/200] [D loss: 0.594034] [G loss: 0.971246]\n",
      "[Epoch 21/200] [D loss: 0.504467] [G loss: 0.742639]\n",
      "[Epoch 22/200] [D loss: 0.481562] [G loss: 0.766631]\n",
      "[Epoch 23/200] [D loss: 0.509075] [G loss: 0.758621]\n",
      "[Epoch 24/200] [D loss: 0.542506] [G loss: 0.696039]\n",
      "[Epoch 25/200] [D loss: 0.583371] [G loss: 1.239398]\n",
      "[Epoch 26/200] [D loss: 0.498841] [G loss: 1.066500]\n",
      "[Epoch 27/200] [D loss: 0.615435] [G loss: 0.436877]\n",
      "[Epoch 28/200] [D loss: 0.583086] [G loss: 0.667457]\n",
      "[Epoch 29/200] [D loss: 0.523236] [G loss: 0.971068]\n",
      "[Epoch 30/200] [D loss: 0.542543] [G loss: 0.952786]\n",
      "[Epoch 31/200] [D loss: 0.569958] [G loss: 0.709563]\n",
      "[Epoch 32/200] [D loss: 0.596633] [G loss: 1.385125]\n",
      "[Epoch 33/200] [D loss: 0.486619] [G loss: 0.814184]\n",
      "[Epoch 34/200] [D loss: 0.458900] [G loss: 1.372477]\n",
      "[Epoch 35/200] [D loss: 0.443163] [G loss: 0.960720]\n",
      "[Epoch 36/200] [D loss: 0.463661] [G loss: 0.827700]\n",
      "[Epoch 37/200] [D loss: 0.429020] [G loss: 1.169181]\n",
      "[Epoch 38/200] [D loss: 0.571297] [G loss: 0.571419]\n",
      "[Epoch 39/200] [D loss: 0.581366] [G loss: 1.620319]\n",
      "[Epoch 40/200] [D loss: 0.407396] [G loss: 1.200580]\n",
      "[Epoch 41/200] [D loss: 0.464199] [G loss: 1.402662]\n",
      "[Epoch 42/200] [D loss: 0.431661] [G loss: 0.771359]\n",
      "[Epoch 43/200] [D loss: 0.815776] [G loss: 0.257019]\n",
      "[Epoch 44/200] [D loss: 0.495365] [G loss: 1.036119]\n",
      "[Epoch 45/200] [D loss: 0.478484] [G loss: 1.109260]\n",
      "[Epoch 46/200] [D loss: 0.516425] [G loss: 1.407131]\n",
      "[Epoch 47/200] [D loss: 0.425671] [G loss: 0.920236]\n",
      "[Epoch 48/200] [D loss: 0.507553] [G loss: 1.177835]\n",
      "[Epoch 49/200] [D loss: 0.525341] [G loss: 0.631212]\n",
      "[Epoch 50/200] [D loss: 0.640499] [G loss: 2.075600]\n",
      "[Epoch 51/200] [D loss: 0.464139] [G loss: 1.463990]\n",
      "[Epoch 52/200] [D loss: 0.358955] [G loss: 1.302832]\n",
      "[Epoch 53/200] [D loss: 0.485472] [G loss: 0.664127]\n",
      "[Epoch 54/200] [D loss: 0.550178] [G loss: 0.460740]\n",
      "[Epoch 55/200] [D loss: 0.361568] [G loss: 1.658870]\n",
      "[Epoch 56/200] [D loss: 0.416003] [G loss: 1.686041]\n",
      "[Epoch 57/200] [D loss: 0.503827] [G loss: 2.397895]\n",
      "[Epoch 58/200] [D loss: 0.338001] [G loss: 1.304033]\n",
      "[Epoch 59/200] [D loss: 0.297342] [G loss: 1.044100]\n",
      "[Epoch 60/200] [D loss: 0.360881] [G loss: 1.278264]\n",
      "[Epoch 61/200] [D loss: 0.411711] [G loss: 1.220175]\n",
      "[Epoch 62/200] [D loss: 0.605948] [G loss: 1.837133]\n",
      "[Epoch 63/200] [D loss: 0.499691] [G loss: 1.788459]\n",
      "[Epoch 64/200] [D loss: 0.524262] [G loss: 1.402624]\n",
      "[Epoch 65/200] [D loss: 0.503275] [G loss: 1.479555]\n",
      "[Epoch 66/200] [D loss: 0.526135] [G loss: 0.676371]\n",
      "[Epoch 67/200] [D loss: 0.515769] [G loss: 0.601920]\n",
      "[Epoch 68/200] [D loss: 0.498935] [G loss: 0.580823]\n",
      "[Epoch 69/200] [D loss: 0.460444] [G loss: 0.968700]\n",
      "[Epoch 70/200] [D loss: 0.415140] [G loss: 0.869151]\n",
      "[Epoch 71/200] [D loss: 0.474744] [G loss: 0.628912]\n",
      "[Epoch 72/200] [D loss: 0.335229] [G loss: 0.989643]\n",
      "[Epoch 73/200] [D loss: 0.367488] [G loss: 1.704669]\n",
      "[Epoch 74/200] [D loss: 0.551115] [G loss: 0.514659]\n",
      "[Epoch 75/200] [D loss: 0.336663] [G loss: 1.543852]\n",
      "[Epoch 76/200] [D loss: 0.399465] [G loss: 1.747463]\n",
      "[Epoch 77/200] [D loss: 0.370558] [G loss: 0.896456]\n",
      "[Epoch 78/200] [D loss: 0.275464] [G loss: 1.263924]\n",
      "[Epoch 79/200] [D loss: 0.635525] [G loss: 0.402558]\n",
      "[Epoch 80/200] [D loss: 0.339530] [G loss: 1.522969]\n",
      "[Epoch 81/200] [D loss: 0.345732] [G loss: 1.410079]\n",
      "[Epoch 82/200] [D loss: 0.357748] [G loss: 0.996781]\n",
      "[Epoch 83/200] [D loss: 0.662289] [G loss: 0.340039]\n",
      "[Epoch 84/200] [D loss: 0.255805] [G loss: 1.336371]\n",
      "[Epoch 85/200] [D loss: 0.277514] [G loss: 1.089856]\n",
      "[Epoch 86/200] [D loss: 0.638913] [G loss: 2.131357]\n",
      "[Epoch 87/200] [D loss: 0.311972] [G loss: 1.370331]\n",
      "[Epoch 88/200] [D loss: 0.353724] [G loss: 1.215311]\n",
      "[Epoch 89/200] [D loss: 0.670515] [G loss: 2.761344]\n",
      "[Epoch 90/200] [D loss: 0.363140] [G loss: 1.300176]\n",
      "[Epoch 91/200] [D loss: 0.676888] [G loss: 2.651968]\n",
      "[Epoch 92/200] [D loss: 0.361219] [G loss: 0.965806]\n",
      "[Epoch 93/200] [D loss: 0.381268] [G loss: 1.097557]\n",
      "[Epoch 94/200] [D loss: 0.570554] [G loss: 0.540141]\n",
      "[Epoch 95/200] [D loss: 0.564622] [G loss: 0.535332]\n",
      "[Epoch 96/200] [D loss: 0.467041] [G loss: 0.786815]\n",
      "[Epoch 97/200] [D loss: 0.367057] [G loss: 0.950702]\n",
      "[Epoch 98/200] [D loss: 0.354431] [G loss: 2.158268]\n",
      "[Epoch 99/200] [D loss: 0.294535] [G loss: 2.580247]\n",
      "[Epoch 100/200] [D loss: 0.371707] [G loss: 1.039600]\n",
      "[Epoch 101/200] [D loss: 0.421665] [G loss: 0.718925]\n",
      "[Epoch 102/200] [D loss: 0.461414] [G loss: 2.082611]\n",
      "[Epoch 103/200] [D loss: 0.455585] [G loss: 0.693982]\n",
      "[Epoch 104/200] [D loss: 0.297514] [G loss: 1.583039]\n",
      "[Epoch 105/200] [D loss: 0.381497] [G loss: 2.308612]\n",
      "[Epoch 106/200] [D loss: 0.451382] [G loss: 3.253256]\n",
      "[Epoch 107/200] [D loss: 0.386429] [G loss: 0.860475]\n",
      "[Epoch 108/200] [D loss: 0.451053] [G loss: 0.676668]\n",
      "[Epoch 109/200] [D loss: 0.376586] [G loss: 0.913912]\n",
      "[Epoch 110/200] [D loss: 0.345224] [G loss: 1.187689]\n",
      "[Epoch 111/200] [D loss: 0.433119] [G loss: 0.670327]\n",
      "[Epoch 112/200] [D loss: 0.257572] [G loss: 1.698119]\n",
      "[Epoch 113/200] [D loss: 0.353541] [G loss: 1.016527]\n",
      "[Epoch 114/200] [D loss: 0.439862] [G loss: 0.767926]\n",
      "[Epoch 115/200] [D loss: 0.529174] [G loss: 1.968724]\n",
      "[Epoch 116/200] [D loss: 0.424165] [G loss: 1.796097]\n",
      "[Epoch 117/200] [D loss: 0.391602] [G loss: 0.883976]\n",
      "[Epoch 118/200] [D loss: 0.421234] [G loss: 2.038110]\n",
      "[Epoch 119/200] [D loss: 0.293396] [G loss: 1.647081]\n",
      "[Epoch 120/200] [D loss: 0.345055] [G loss: 1.908966]\n",
      "[Epoch 121/200] [D loss: 0.257154] [G loss: 1.572878]\n",
      "[Epoch 122/200] [D loss: 0.292958] [G loss: 1.786321]\n",
      "[Epoch 123/200] [D loss: 0.903934] [G loss: 3.573622]\n",
      "[Epoch 124/200] [D loss: 0.327942] [G loss: 1.036346]\n",
      "[Epoch 125/200] [D loss: 0.504498] [G loss: 0.589507]\n",
      "[Epoch 126/200] [D loss: 0.355594] [G loss: 1.883904]\n",
      "[Epoch 127/200] [D loss: 0.277572] [G loss: 1.774428]\n",
      "[Epoch 128/200] [D loss: 0.329572] [G loss: 1.102093]\n",
      "[Epoch 129/200] [D loss: 0.260455] [G loss: 1.729559]\n",
      "[Epoch 130/200] [D loss: 0.158553] [G loss: 2.028128]\n",
      "[Epoch 131/200] [D loss: 0.298334] [G loss: 1.271730]\n",
      "[Epoch 132/200] [D loss: 0.396467] [G loss: 1.725376]\n",
      "[Epoch 133/200] [D loss: 0.281341] [G loss: 1.888045]\n",
      "[Epoch 134/200] [D loss: 0.326207] [G loss: 1.168375]\n",
      "[Epoch 135/200] [D loss: 0.267383] [G loss: 1.216054]\n",
      "[Epoch 136/200] [D loss: 0.281694] [G loss: 1.144310]\n",
      "[Epoch 137/200] [D loss: 0.403969] [G loss: 0.719905]\n",
      "[Epoch 138/200] [D loss: 0.263858] [G loss: 1.658693]\n",
      "[Epoch 139/200] [D loss: 0.618505] [G loss: 3.291948]\n",
      "[Epoch 140/200] [D loss: 0.338765] [G loss: 0.939032]\n",
      "[Epoch 141/200] [D loss: 0.391973] [G loss: 0.998772]\n",
      "[Epoch 142/200] [D loss: 0.331020] [G loss: 1.697557]\n",
      "[Epoch 143/200] [D loss: 0.645913] [G loss: 0.364555]\n",
      "[Epoch 144/200] [D loss: 0.275501] [G loss: 1.665363]\n",
      "[Epoch 145/200] [D loss: 0.255279] [G loss: 1.467922]\n",
      "[Epoch 146/200] [D loss: 0.272631] [G loss: 1.597812]\n",
      "[Epoch 147/200] [D loss: 0.351265] [G loss: 2.349846]\n",
      "[Epoch 148/200] [D loss: 0.540388] [G loss: 0.489252]\n",
      "[Epoch 149/200] [D loss: 0.257275] [G loss: 1.897940]\n",
      "[Epoch 150/200] [D loss: 0.373726] [G loss: 2.272080]\n",
      "[Epoch 151/200] [D loss: 0.221510] [G loss: 1.954253]\n",
      "[Epoch 152/200] [D loss: 0.584556] [G loss: 0.489556]\n",
      "[Epoch 153/200] [D loss: 0.320002] [G loss: 1.042238]\n",
      "[Epoch 154/200] [D loss: 0.271595] [G loss: 1.903907]\n",
      "[Epoch 155/200] [D loss: 0.349603] [G loss: 1.252291]\n",
      "[Epoch 156/200] [D loss: 0.467259] [G loss: 0.660488]\n",
      "[Epoch 157/200] [D loss: 0.322756] [G loss: 1.061277]\n",
      "[Epoch 158/200] [D loss: 0.382088] [G loss: 2.044178]\n",
      "[Epoch 159/200] [D loss: 0.303175] [G loss: 1.534487]\n",
      "[Epoch 160/200] [D loss: 0.224508] [G loss: 1.656826]\n",
      "[Epoch 161/200] [D loss: 0.167419] [G loss: 2.642725]\n",
      "[Epoch 162/200] [D loss: 0.535677] [G loss: 3.313161]\n",
      "[Epoch 163/200] [D loss: 0.252846] [G loss: 1.152869]\n",
      "[Epoch 164/200] [D loss: 0.275924] [G loss: 1.169028]\n",
      "[Epoch 165/200] [D loss: 0.317325] [G loss: 1.059725]\n",
      "[Epoch 166/200] [D loss: 0.563321] [G loss: 4.678512]\n",
      "[Epoch 167/200] [D loss: 0.289197] [G loss: 2.111382]\n",
      "[Epoch 168/200] [D loss: 0.455000] [G loss: 2.097507]\n",
      "[Epoch 169/200] [D loss: 0.344745] [G loss: 2.067856]\n",
      "[Epoch 170/200] [D loss: 0.443201] [G loss: 3.286364]\n",
      "[Epoch 171/200] [D loss: 0.505765] [G loss: 0.547174]\n",
      "[Epoch 172/200] [D loss: 0.253420] [G loss: 1.691258]\n",
      "[Epoch 173/200] [D loss: 0.370590] [G loss: 2.242878]\n",
      "[Epoch 174/200] [D loss: 0.536339] [G loss: 2.985448]\n",
      "[Epoch 175/200] [D loss: 0.337077] [G loss: 2.069396]\n",
      "[Epoch 176/200] [D loss: 0.548294] [G loss: 3.511430]\n",
      "[Epoch 177/200] [D loss: 0.286182] [G loss: 1.639724]\n",
      "[Epoch 178/200] [D loss: 0.273602] [G loss: 1.926423]\n",
      "[Epoch 179/200] [D loss: 0.268452] [G loss: 1.796924]\n",
      "[Epoch 180/200] [D loss: 0.313805] [G loss: 1.246133]\n",
      "[Epoch 181/200] [D loss: 0.637785] [G loss: 3.187157]\n",
      "[Epoch 182/200] [D loss: 0.367161] [G loss: 1.974646]\n",
      "[Epoch 183/200] [D loss: 0.330095] [G loss: 1.338089]\n",
      "[Epoch 184/200] [D loss: 0.301454] [G loss: 1.366297]\n",
      "[Epoch 185/200] [D loss: 0.312484] [G loss: 1.225584]\n",
      "[Epoch 186/200] [D loss: 0.365484] [G loss: 0.899546]\n",
      "[Epoch 187/200] [D loss: 0.445501] [G loss: 2.918168]\n",
      "[Epoch 188/200] [D loss: 0.288842] [G loss: 1.633873]\n",
      "[Epoch 189/200] [D loss: 0.273192] [G loss: 1.536083]\n",
      "[Epoch 190/200] [D loss: 0.532550] [G loss: 0.512910]\n",
      "[Epoch 191/200] [D loss: 0.194679] [G loss: 2.224990]\n",
      "[Epoch 192/200] [D loss: 0.427707] [G loss: 0.727396]\n",
      "[Epoch 193/200] [D loss: 0.300308] [G loss: 2.336882]\n",
      "[Epoch 194/200] [D loss: 0.274023] [G loss: 1.273512]\n",
      "[Epoch 195/200] [D loss: 0.374477] [G loss: 0.825396]\n",
      "[Epoch 196/200] [D loss: 0.291353] [G loss: 2.618183]\n",
      "[Epoch 197/200] [D loss: 0.205725] [G loss: 1.540230]\n",
      "[Epoch 198/200] [D loss: 0.379668] [G loss: 0.821224]\n",
      "[Epoch 199/200] [D loss: 0.441370] [G loss: 4.027272]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(imgs.size(0),1, requires_grad=False, device=device)\n",
    "        fake = torch.zeros(imgs.size(0),1, requires_grad=False, device=device)\n",
    "        \n",
    "        real_imgs = imgs.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        # Train Generator\n",
    "        optim_G.zero_grad()\n",
    "        z = torch.randn([imgs.shape[0], 100], device=device)   # 100: latent_dim\n",
    "        gen_imgs = G(z)\n",
    "        g_loss = loss_function(D(gen_imgs), valid)   # Discriminator를 진짜라고 속이는 방향으로 학습\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optim_G.step()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optim_D.zero_grad()\n",
    "        \n",
    "        real_loss = loss_function(D(real_imgs), valid)\n",
    "        fake_loss = loss_function(D(gen_imgs.detach()), fake)\n",
    "        \n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optim_D.step()\n",
    "        \n",
    "        \n",
    "    print(\n",
    "        \"[Epoch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "        % (epoch, n_epochs, d_loss.item(), g_loss.item())\n",
    "    )\n",
    "\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_image(gen_imgs.data[:25], f\"../../data/gan/{epoch+1}.png\", nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
